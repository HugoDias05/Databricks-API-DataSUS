{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ce69c38-fdd6-48a0-b9b1-500285c1755f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üì• Bronze Layer - Ingest√£o de Dados SIH (DATASUS)\n",
    "####**Fonte**: Sistema de Informa√ß√µes Hospitalares (SIH/SUS)\n",
    "####**Descri√ß√£o**: Dados de interna√ß√µes hospitalares do Brasil\n",
    "####**Per√≠odo**: √öltimos 12 meses dispon√≠veis\n",
    "####**Formato Original**: DBC (comprimido) ‚Üí Convers√£o para Parquet/Delta\n",
    "## üìä Estrutura dos Dados SIH:\n",
    " - **AIH**: Autoriza√ß√£o de Interna√ß√£o Hospitalar\n",
    " - Procedimentos realizados\n",
    " - Diagn√≥sticos (CID-10)\n",
    " - Munic√≠pio/Estado\n",
    " - Valores pagos\n",
    " - Tempo de perman√™ncia\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "804fe21e-37b1-483c-89cb-24f810c2a102",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1Ô∏è‚É£ Configura√ß√£o Inicial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2547dfb8-80ba-44e4-8eeb-c7bc514dd7e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import time\n",
    "import io\n",
    "\n",
    "print(\"‚úÖ Bibliotecas importadas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c35a8e65-9b39-430c-903d-d958252cd8a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configurar catalog e schema\n",
    "catalog_name = \"datasus_project\"\n",
    "schema_name = \"bronze\"\n",
    "\n",
    "# Usar catalog/schema\n",
    "spark.sql(f\"USE CATALOG {catalog_name}\")\n",
    "spark.sql(f\"USE SCHEMA {schema_name}\")\n",
    "\n",
    "print(f\"‚úÖ Usando: {catalog_name}.{schema_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57efc600-241c-4041-92d1-0674a06aa239",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2Ô∏è‚É£ Explorar API OpenDataSUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e18c0c5f-14dd-4b3f-9e73-c5403f1e3e69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def list_all_datasets():\n",
    "    \"\"\"\n",
    "    Lista TODOS os datasets dispon√≠veis no OpenDataSUS\n",
    "    A API CKAN retorna apenas 10 por padr√£o, precisamos aumentar o 'rows'\n",
    "    \"\"\"\n",
    "    base_url = \"https://opendatasus.saude.gov.br/api/3/action/package_search\"\n",
    "    \n",
    "    # Primeiro, descobrir quantos datasets existem no total\n",
    "    params = {\n",
    "        \"rows\": 0  # Retorna apenas o count\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, params=params, timeout=15)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            total_datasets = data.get(\"result\", {}).get(\"count\", 0)\n",
    "            print(f\"üìä Total de datasets dispon√≠veis: {total_datasets}\")\n",
    "            \n",
    "            # Agora buscar todos os datasets\n",
    "            params[\"rows\"] = total_datasets\n",
    "            response = requests.get(base_url, params=params, timeout=30)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return data.get(\"result\", {}).get(\"results\", [])\n",
    "        else:\n",
    "            print(f\"‚ùå Erro: Status {response.status_code}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro na requisi√ß√£o: {e}\")\n",
    "        return []\n",
    "\n",
    "# Buscar todos os datasets\n",
    "print(\"üîç Buscando TODOS os datasets do OpenDataSUS...\")\n",
    "all_datasets = list_all_datasets()\n",
    "\n",
    "print(f\"\\n‚úÖ Total de datasets encontrados: {len(all_datasets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45c79c68-7fd6-4b54-a849-5594345c4f98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mostrar os 10 primeiros datasets\n",
    "print(\"\\nüìã PRIMEIROS 10 DATASETS DISPON√çVEIS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, ds in enumerate(all_datasets[:10]):\n",
    "    print(f\"\\n{i+1}. {ds.get('title', 'Sem t√≠tulo')}\")\n",
    "    print(f\"   ID: {ds.get('name', 'N/A')}\")\n",
    "    print(f\"   Organiza√ß√£o: {ds.get('organization', {}).get('title', 'N/A')}\")\n",
    "    print(f\"   Recursos: {len(ds.get('resources', []))}\")\n",
    "    \n",
    "    # Mostrar formatos dispon√≠veis\n",
    "    resources = ds.get('resources', [])\n",
    "    if resources:\n",
    "        formats = [r.get('format', 'N/A') for r in resources]\n",
    "        print(f\"   Formatos: {', '.join(set(formats))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3af9fa2e-c8a7-4998-8f63-9de832fa6c5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3Ô∏è‚É£ Buscar Dataset Espec√≠fico - CNES (Estabelecimentos de Sa√∫de)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6731ee48-21a7-4a58-892b-f6c7fc9efa6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def find_dataset_by_keyword(datasets, keyword):\n",
    "    \"\"\"\n",
    "    Busca datasets que contenham determinada palavra-chave\n",
    "    \"\"\"\n",
    "    matches = []\n",
    "    for ds in datasets:\n",
    "        title = ds.get('title', '').lower()\n",
    "        name = ds.get('name', '').lower()\n",
    "        if keyword.lower() in title or keyword.lower() in name:\n",
    "            matches.append(ds)\n",
    "    return matches\n",
    "\n",
    "# Buscar dataset do CNES\n",
    "print(\"üîç Buscando dataset CNES...\")\n",
    "cnes_datasets = find_dataset_by_keyword(all_datasets, \"CNES\")\n",
    "\n",
    "print(f\"\\n‚úÖ Encontrados {len(cnes_datasets)} datasets relacionados ao CNES\")\n",
    "\n",
    "for i, ds in enumerate(cnes_datasets):\n",
    "    print(f\"\\n{i+1}. {ds.get('title')}\")\n",
    "    print(f\"   ID: {ds.get('name')}\")\n",
    "    print(f\"   Recursos: {len(ds.get('resources', []))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a654a10f-1ac9-4cce-9334-080cf33f9e90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 4Ô∏è‚É£ Obter Recursos (Arquivos) do Dataset CNES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddb0937c-91f2-4845-a799-c7b713893bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Pegar o primeiro dataset do CNES\n",
    "if cnes_datasets:\n",
    "    cnes_dataset = cnes_datasets[0]\n",
    "    resources = cnes_dataset.get('resources', [])\n",
    "    \n",
    "    print(f\"üì¶ Dataset: {cnes_dataset.get('title')}\")\n",
    "    print(f\"üìã Total de recursos: {len(resources)}\\n\")\n",
    "    \n",
    "    print(\"üóÇÔ∏è RECURSOS DISPON√çVEIS:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    csv_resources = []\n",
    "    for i, resource in enumerate(resources[:20]):  # Mostrar primeiros 20\n",
    "        format_type = resource.get('format', 'N/A')\n",
    "        name = resource.get('name', 'Sem nome')\n",
    "        url = resource.get('url', 'N/A')\n",
    "        \n",
    "        print(f\"\\n{i+1}. {name}\")\n",
    "        print(f\"   Formato: {format_type}\")\n",
    "        print(f\"   URL: {url[:100]}...\")\n",
    "        \n",
    "        # Guardar recursos CSV\n",
    "        if format_type.upper() in ['CSV', 'JSON', 'PARQUET']:\n",
    "            csv_resources.append(resource)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Recursos em formato estruturado: {len(csv_resources)}\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dataset CNES encontrado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b103492b-ded5-4b70-bdb4-9f6bb5e3a6c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 5Ô∏è‚É£ Download e Ingest√£o de Dados REAIS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6b1f6ec-3b0d-4c03-8bb3-0de8cf743b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "from io import BytesIO\n",
    "\n",
    "if csv_resources:\n",
    "    selected_resource = csv_resources[0]\n",
    "    \n",
    "    print(f\"üì• Baixando recurso: {selected_resource.get('name')}\")\n",
    "    print(f\"   URL: {selected_resource.get('url')}\")\n",
    "    \n",
    "    try:\n",
    "        print(\"\\n‚è≥ Fazendo download...\")\n",
    "        response = requests.get(selected_resource.get('url'), timeout=120)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            file_size_mb = len(response.content) / (1024 * 1024)\n",
    "            print(f\"‚úÖ Download conclu√≠do! Tamanho: {file_size_mb:.2f} MB\")\n",
    "            \n",
    "            if selected_resource.get('url').endswith('.zip'):\n",
    "                print(\"\\nüì¶ Descompactando ZIP...\")\n",
    "                \n",
    "                try:\n",
    "                    zip_file = zipfile.ZipFile(BytesIO(response.content))\n",
    "                    file_list = zip_file.namelist()\n",
    "                    csv_file = [f for f in file_list if f.endswith('.csv')][0]\n",
    "                    \n",
    "                    print(f\"üìÑ Processando: {csv_file}\")\n",
    "                    print(\"‚è≥ Lendo CSV... (pode levar 1-2 minutos)\")\n",
    "                    \n",
    "                    with zip_file.open(csv_file) as f:\n",
    "                        # Tentar com ponto-e-v√≠rgula (padr√£o brasileiro)\n",
    "                        try:\n",
    "                            df_pandas = pd.read_csv(\n",
    "                                f, \n",
    "                                nrows=20000,\n",
    "                                encoding='latin-1',\n",
    "                                sep=';',\n",
    "                                on_bad_lines='skip',\n",
    "                                engine='python',  # Removido low_memory\n",
    "                                quotechar='\"'\n",
    "                            )\n",
    "                            \n",
    "                            print(f\"\\n‚úÖ Sucesso com separador ';'\")\n",
    "                            print(f\"   Shape: {df_pandas.shape}\")\n",
    "                            \n",
    "                        except Exception as e1:\n",
    "                            print(f\"‚ö†Ô∏è Erro com ';': {e1}\")\n",
    "                            print(\"üí° Tentando com v√≠rgula...\")\n",
    "                            \n",
    "                            # Resetar o arquivo\n",
    "                            f.seek(0)\n",
    "                            \n",
    "                            df_pandas = pd.read_csv(\n",
    "                                f,\n",
    "                                nrows=20000,\n",
    "                                encoding='latin-1',\n",
    "                                sep=',',\n",
    "                                on_bad_lines='skip',\n",
    "                                engine='python',\n",
    "                                quotechar='\"'\n",
    "                            )\n",
    "                            \n",
    "                            print(f\"\\n‚úÖ Sucesso com separador ','\")\n",
    "                            print(f\"   Shape: {df_pandas.shape}\")\n",
    "                        \n",
    "                        # Se chegou aqui, deu certo!\n",
    "                        print(f\"\\nüìä DADOS CARREGADOS COM SUCESSO!\")\n",
    "                        print(f\"   Registros: {len(df_pandas):,}\")\n",
    "                        print(f\"   Colunas: {len(df_pandas.columns)}\")\n",
    "                        \n",
    "                        # Limpar nomes de colunas (remover espa√ßos)\n",
    "                        df_pandas.columns = df_pandas.columns.str.strip()\n",
    "                        \n",
    "                        print(f\"\\nüîç Primeiras 15 colunas:\")\n",
    "                        for i, col in enumerate(df_pandas.columns[:15]):\n",
    "                            print(f\"   {i+1}. {col}\")\n",
    "                        \n",
    "                        if len(df_pandas.columns) > 15:\n",
    "                            print(f\"   ... e mais {len(df_pandas.columns) - 15} colunas\")\n",
    "                        \n",
    "                        print(f\"\\nüìã Primeiras linhas:\")\n",
    "                        display(df_pandas.head())\n",
    "                        \n",
    "                        # Se tiver coluna de UF/Estado\n",
    "                        uf_cols = [col for col in df_pandas.columns if 'UF' in col.upper() or 'ESTADO' in col.upper()]\n",
    "                        if uf_cols:\n",
    "                            print(f\"\\nüìç Distribui√ß√£o por Estado (coluna: {uf_cols[0]}):\")\n",
    "                            print(df_pandas[uf_cols[0]].value_counts().head(10))\n",
    "                        \n",
    "                        # Informa√ß√µes de tipos\n",
    "                        print(f\"\\nüîß Tipos de dados:\")\n",
    "                        print(df_pandas.dtypes.value_counts())\n",
    "                        \n",
    "                        # Guardar para usar depois\n",
    "                        df_cnes_real = df_pandas.copy()\n",
    "                        print(\"\\n‚úÖ Dados REAIS do CNES prontos para uso!\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"\\n‚ùå Erro final ao processar CSV: {e}\")\n",
    "                    print(f\"   Tipo: {type(e).__name__}\")\n",
    "                    print(\"\\nüí° N√£o se preocupe! Vamos usar dados sint√©ticos de alta qualidade.\")\n",
    "                    df_cnes_real = None\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no download: {e}\")\n",
    "        df_cnes_real = None\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum recurso CSV encontrado\")\n",
    "    df_cnes_real = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "331a5a4e-80b0-46ff-83bd-9ca65eaa93d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 6Ô∏è‚É£ Salvar Dados (Real ou Sint√©tico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2c4a5ad-6a6a-4d7e-b21a-b526aad2ae5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Verificar se temos dados reais\n",
    "usar_dados_reais = 'df_cnes_real' in locals() and df_cnes_real is not None and len(df_cnes_real) > 0\n",
    "\n",
    "if usar_dados_reais:\n",
    "    print(\"=\"*70)\n",
    "    print(\"üéâ PROCESSANDO DADOS REAIS DO DATASUS - CNES!\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Converter para Spark\n",
    "    print(\"\\nüîÑ Convertendo para PySpark DataFrame...\")\n",
    "    df_spark = spark.createDataFrame(df_cnes_real)\n",
    "    \n",
    "    # Adicionar metadados\n",
    "    df_spark = df_spark \\\n",
    "        .withColumn(\"data_ingestao\", F.current_timestamp()) \\\n",
    "        .withColumn(\"fonte\", F.lit(\"datasus_cnes_oficial\")) \\\n",
    "        .withColumn(\"versao\", F.lit(\"2025\"))\n",
    "    \n",
    "    record_count = df_spark.count()\n",
    "    print(f\"‚úÖ Convers√£o completa: {record_count:,} registros\")\n",
    "    \n",
    "    # Salvar\n",
    "    table_name = f\"{catalog_name}.{schema_name}.cnes_estabelecimentos_raw\"\n",
    "    \n",
    "    print(f\"\\nüíæ Salvando em Delta Table: {table_name}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_spark.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .saveAsTable(table_name)\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Salvo com sucesso!\")\n",
    "    print(f\"   Tempo de salvamento: {exec_time:.2f}s\")\n",
    "    print(f\"   Tabela: {table_name}\")\n",
    "    \n",
    "    # Log\n",
    "    def log_pipeline_execution(pipeline_name, status, records=0, exec_time=0, error=None):\n",
    "        schema_control = StructType([\n",
    "            StructField(\"pipeline_name\", StringType(), False),\n",
    "            StructField(\"execution_date\", TimestampType(), False),\n",
    "            StructField(\"status\", StringType(), False),\n",
    "            StructField(\"records_processed\", LongType(), True),\n",
    "            StructField(\"execution_time_seconds\", DoubleType(), True),\n",
    "            StructField(\"error_message\", StringType(), True)\n",
    "        ])\n",
    "        log_data = [(pipeline_name, datetime.now(), status, records, exec_time, error)]\n",
    "        df_log = spark.createDataFrame(log_data, schema_control)\n",
    "        df_log.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{catalog_name}.bronze.pipeline_control\")\n",
    "    \n",
    "    log_pipeline_execution(\"bronze_ingest_cnes_real\", \"SUCCESS\", record_count, exec_time)\n",
    "    \n",
    "    # Mostrar resultado\n",
    "    print(f\"\\nüìä Amostra dos dados salvos:\")\n",
    "    spark.table(table_name).show(5, truncate=True)\n",
    "    \n",
    "    print(f\"\\n‚úÖ DADOS REAIS INGERIDOS COM SUCESSO!\")\n",
    "\n",
    "else:\n",
    "    print(\"=\"*70)\n",
    "    print(\"üí° GERANDO DADOS SINT√âTICOS DE ALTA QUALIDADE\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nBaseado na estrutura real do SIH/DATASUS\")\n",
    "    print(\"Perfeito para demonstrar pipeline completo!\\n\")\n",
    "    \n",
    "    # Gerar dados sint√©ticos\n",
    "    import random\n",
    "    from datetime import datetime, timedelta\n",
    "    \n",
    "    def generate_synthetic_sih_data(num_records=15000):\n",
    "        print(f\"üîÑ Gerando {num_records:,} registros sint√©ticos...\")\n",
    "        \n",
    "        estados = ['PE', 'BA', 'CE', 'MA', 'PB', 'RN', 'AL', 'SE', 'PI']\n",
    "        \n",
    "        municipios = {\n",
    "            'PE': [('261160', 'Recife'), ('260545', 'Jaboat√£o'), ('260410', 'Olinda')],\n",
    "            'BA': [('292740', 'Salvador'), ('291080', 'Feira de Santana')],\n",
    "            'CE': [('230440', 'Fortaleza'), ('230765', 'Juazeiro do Norte')],\n",
    "        }\n",
    "        \n",
    "        procedimentos = [\n",
    "            ('0303010029', 'Parto normal', 1500),\n",
    "            ('0303010037', 'Parto cesariano', 2500),\n",
    "            ('0303040114', 'Tratamento de pneumonia', 3500),\n",
    "            ('0303050012', 'Tratamento de AVC', 8000),\n",
    "            ('0303090022', 'Tratamento de fratura', 5000),\n",
    "            ('0303080014', 'Tratamento card√≠aco', 7000),\n",
    "        ]\n",
    "        \n",
    "        cids = [\n",
    "            ('O80', 'Parto √∫nico espont√¢neo'),\n",
    "            ('O82', 'Parto por cesariana'),\n",
    "            ('J18', 'Pneumonia n√£o especificada'),\n",
    "            ('I64', 'AVC n√£o especificado'),\n",
    "            ('S72', 'Fratura do f√™mur'),\n",
    "            ('I50', 'Insufici√™ncia card√≠aca'),\n",
    "        ]\n",
    "        \n",
    "        data = []\n",
    "        start_date = datetime.now() - timedelta(days=365)\n",
    "        \n",
    "        for i in range(num_records):\n",
    "            uf = random.choice(estados)\n",
    "            \n",
    "            if uf in municipios:\n",
    "                munic_info = random.choice(municipios[uf])\n",
    "                munic_cod = munic_info[0]\n",
    "                munic_nome = munic_info[1]\n",
    "            else:\n",
    "                munic_cod = f\"00{random.randint(1000,9999)}\"\n",
    "                munic_nome = \"Munic√≠pio Gen√©rico\"\n",
    "            \n",
    "            dt_inter = start_date + timedelta(days=random.randint(0, 365))\n",
    "            dias_perm = random.randint(1, 30)\n",
    "            dt_saida = dt_inter + timedelta(days=dias_perm)\n",
    "            \n",
    "            proc = random.choice(procedimentos)\n",
    "            cid = random.choice(cids)\n",
    "            \n",
    "            val_base = proc[2]\n",
    "            val_total = val_base + random.uniform(-500, 1000) + (dias_perm * 200)\n",
    "            \n",
    "            record = {\n",
    "                'N_AIH': f\"{random.randint(10000000000, 99999999999)}\",\n",
    "                'CNES': f\"{random.randint(2000000, 7999999)}\",\n",
    "                'MUNIC_RES_COD': munic_cod,\n",
    "                'MUNIC_RES_NOME': munic_nome,\n",
    "                'UF': uf,\n",
    "                'PROC_REA': proc[0],\n",
    "                'PROC_DESC': proc[1],\n",
    "                'DIAG_PRINC': cid[0],\n",
    "                'DIAG_DESC': cid[1],\n",
    "                'DT_INTER': dt_inter.strftime('%Y-%m-%d'),\n",
    "                'DT_SAIDA': dt_saida.strftime('%Y-%m-%d'),\n",
    "                'DIAS_PERM': dias_perm,\n",
    "                'VAL_TOT': round(val_total, 2),\n",
    "                'IDADE': random.randint(0, 100),\n",
    "                'SEXO': random.choice(['M', 'F']),\n",
    "                'MARCA_UTI': random.choice(['Sim', 'N√£o']),\n",
    "                'QT_DIARIAS': dias_perm,\n",
    "                'ANO_CMPT': dt_inter.year,\n",
    "                'MES_CMPT': dt_inter.month,\n",
    "                'TRIMESTRE': ((dt_inter.month - 1) // 3) + 1,\n",
    "            }\n",
    "            \n",
    "            data.append(record)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # Gerar\n",
    "    print()\n",
    "    synthetic_data = generate_synthetic_sih_data(15000)\n",
    "    df_pandas = pd.DataFrame(synthetic_data)\n",
    "    \n",
    "    print(f\"‚úÖ Dados gerados com sucesso!\")\n",
    "    print(f\"   Shape: {df_pandas.shape}\")\n",
    "    print(f\"\\nüìã Preview:\")\n",
    "    display(df_pandas.head())\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    print(f\"\\nüìä Estat√≠sticas do dataset sint√©tico:\")\n",
    "    print(f\"   Estados √∫nicos: {df_pandas['UF'].nunique()}\")\n",
    "    print(f\"   Munic√≠pios: {df_pandas['MUNIC_RES_COD'].nunique()}\")\n",
    "    print(f\"   Procedimentos: {df_pandas['PROC_REA'].nunique()}\")\n",
    "    print(f\"   Valor total: R$ {df_pandas['VAL_TOT'].sum():,.2f}\")\n",
    "    print(f\"   M√©dia de perman√™ncia: {df_pandas['DIAS_PERM'].mean():.1f} dias\")\n",
    "    \n",
    "    # Converter para Spark\n",
    "    print(f\"\\nüîÑ Convertendo para PySpark...\")\n",
    "    df_spark = spark.createDataFrame(df_pandas)\n",
    "    \n",
    "    df_spark = df_spark \\\n",
    "        .withColumn(\"data_ingestao\", F.current_timestamp()) \\\n",
    "        .withColumn(\"fonte\", F.lit(\"synthetic_sih_v2\")) \\\n",
    "        .withColumn(\"versao\", F.lit(\"2.0\"))\n",
    "    \n",
    "    record_count = df_spark.count()\n",
    "    print(f\"‚úÖ Convers√£o completa: {record_count:,} registros\")\n",
    "    \n",
    "    # Salvar\n",
    "    table_name = f\"{catalog_name}.{schema_name}.sih_internacoes_raw\"\n",
    "    \n",
    "    print(f\"\\nüíæ Salvando em: {table_name}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_spark.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"overwriteSchema\", \"true\") \\\n",
    "        .partitionBy(\"ANO_CMPT\", \"MES_CMPT\") \\\n",
    "        .saveAsTable(table_name)\n",
    "    \n",
    "    exec_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"‚úÖ Salvo em {exec_time:.2f}s!\")\n",
    "    \n",
    "    # Log\n",
    "    def log_pipeline_execution(pipeline_name, status, records=0, exec_time=0, error=None):\n",
    "        schema_control = StructType([\n",
    "            StructField(\"pipeline_name\", StringType(), False),\n",
    "            StructField(\"execution_date\", TimestampType(), False),\n",
    "            StructField(\"status\", StringType(), False),\n",
    "            StructField(\"records_processed\", LongType(), True),\n",
    "            StructField(\"execution_time_seconds\", DoubleType(), True),\n",
    "            StructField(\"error_message\", StringType(), True)\n",
    "        ])\n",
    "        log_data = [(pipeline_name, datetime.now(), status, records, exec_time, error)]\n",
    "        df_log = spark.createDataFrame(log_data, schema_control)\n",
    "        df_log.write.format(\"delta\").mode(\"append\").saveAsTable(f\"{catalog_name}.bronze.pipeline_control\")\n",
    "    \n",
    "    log_pipeline_execution(\"bronze_ingest_sih_synthetic\", \"SUCCESS\", record_count, exec_time)\n",
    "    \n",
    "    print(f\"\\nüìä Amostra dos dados salvos:\")\n",
    "    spark.table(table_name).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52bc4c1a-fb8c-4ed0-a796-eea0f5815f32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üéâ Resumo da Ingest√£o Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20019a60-6d47-4cdf-a2f7-ed03adea8f42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"üéâ BRONZE LAYER - INGEST√ÉO CONCLU√çDA COM SUCESSO!\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Listar tabelas\n",
    "print(\"\\nüìã TABELAS BRONZE:\")\n",
    "spark.sql(f\"SHOW TABLES IN {catalog_name}.{schema_name}\").show(truncate=False)\n",
    "\n",
    "# Ver logs\n",
    "print(\"\\nüìä LOGS DE EXECU√á√ÉO:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        pipeline_name,\n",
    "        DATE_FORMAT(execution_date, 'yyyy-MM-dd HH:mm:ss') as exec_date,\n",
    "        status,\n",
    "        FORMAT_NUMBER(records_processed, 0) as records,\n",
    "        ROUND(execution_time_seconds, 2) as exec_time_sec\n",
    "    FROM {catalog_name}.bronze.pipeline_control\n",
    "    ORDER BY execution_date DESC\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üöÄ PR√ìXIMO PASSO: SILVER LAYER\")\n",
    "print(\"   - Limpeza e padroniza√ß√£o\")\n",
    "print(\"   - Enriquecimento de dados\")\n",
    "print(\"   - Valida√ß√µes de qualidade\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_ingest_sih_datasus",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
